
from jetts.prompts import get_partial_response
from ..data import Query, Response

single_instance_rate_max_score = 5

PROMPT_PAIRWISE_COMPARISON="""
You are a helpful assistant in evaluating the quality of the responses for a given instruction. Your goal is to select the best response for the given instruction.
Select Response A or Response B, that is better for the given instruction. The two responses are generated by two different AI chatbots respectively.
Do NOT say both / neither are good.

Here are some rules of the evaluation:
(1) You should prioritize evaluating whether the output honestly/precisely/closely executes the instruction, then consider its helpfulness, accuracy, level of detail, harmlessness, etc.
(2) Responses should NOT contain more/less than what the instruction asks for, as such responses do NOT precisely execute the instruction.
(3) You should avoid any potential bias and your judgment should be as objective as possible. Here are some potential sources of bias:
- The order in which the responses were presented should NOT affect your judgment, as Response A and Response B are **equally likely** to be the better.
- The length of the responses should NOT affect your judgement, as a longer response does not necessarily correspond to a better response. When making your decision, evaluate if the response length is appropriate for the given instruction.{partial_response_note}

Your reply should strictly follow this format:
**Reasoning:** <feedback evaluating the responses>

**Result:** <A or B>

Here is the data.

Instruction:
```
{query_text}
```

Response A:
```
{response_A}
```

Response B:
```
{response_B}
```
""".strip()


PROMPT_PAIRWISE_COMPARISON_MATH="""
You are a helpful assistant in evaluating the quality of the responses for a given instruction, which is a math problem. Your goal is to select the best response for the given instruction.
Select Response A or Response B, that is better for the given math problem. The two responses are generated by two different AI chatbots respectively.
Do NOT say both / neither are good.

Here are some rules of the evaluation:
(1) You should prioritize evaluating if the output arrives at the correct solution for the given math problem and if the logical reasoning to derive the solution is sound.
(2) If both responses arrive at the correct solution, choose the response that contains the better logical reasoning. For reasoning, prioritize correctness, then completeness and conciseness.
(3) Responses should NOT contain more/less than what the question asks for, as such responses do NOT precisely solve the problem.
(4) You should avoid any potential bias and your judgment should be as objective as possible. Here are some potential sources of bias:
- The order in which the responses were presented should NOT affect your judgment, as Response A and Response B are **equally likely** to be the better.
- The length of the responses should NOT affect your judgement, as a longer response does not necessarily correspond to a better response. When making your decision, evaluate if the response length is appropriate for the given instruction.{partial_response_note}

Your reply should strictly follow this format:
**Reasoning:** <feedback evaluating the responses>

**Result:** <A or B>

Here is the data.

Instruction:
```
{query_text}
```

Response A:
```
{response_A}
```

Response B:
```
{response_B}
```
""".strip()

PROMPT_PAIRWISE_COMPARISON_CODE="""
You are a helpful assistant in evaluating the quality of the responses for a given instruction, which is a coding problem. Your goal is to select the best response for the given instruction.
Select Response A or Response B, that is better for the given coding problem. The two responses are generated by two different AI chatbots respectively.
Do NOT say both / neither are good.

Here are some rules of the evaluation:
(1) You should prioritize evaluating if the output response code correctly implements the desired functionality in the instruction.
(2) If both responses correctly implement the desired functionality, choose the response that contains the better written code. Prioritize conciseness and readibility.
(3) Responses should NOT contain more/less than what the question asks for, as such responses do NOT precisely solve the problem.
(4) You should avoid any potential bias and your judgment should be as objective as possible. Here are some potential sources of bias:
- The order in which the responses were presented should NOT affect your judgment, as Response A and Response B are **equally likely** to be the better.
- The length of the responses should NOT affect your judgement, as a longer response does not necessarily correspond to a better response. When making your decision, evaluate if the response length is appropriate for the given instruction.{partial_response_note}

Your reply should strictly follow this format:
**Reasoning:** <feedback evaluating the responses>

**Result:** <A or B>

Here is the data.

Instruction:
```
{query_text}
```

Response A:
```
{response_A}
```

Response B:
```
{response_B}
```
""".strip()


def parse_pairwise_judgment(judge_output, return_critique=False):
    critique_judgement = judge_output.split('**Result:**')
    critique = critique_judgement[0].replace('**Reasoning:**', '').strip()
    judgement = critique_judgement[-1].strip()
    if return_critique:
        if judgement == 'A':
            return 0, critique
        elif judgement == 'B':
            return 1, critique
        else:
            return None, None
    else:
        if judgement == 'A':
            return 0
        elif judgement == 'B':
            return 1
        else:
            return None


PROMPT_SINGLE_RATING="""
You are tasked with evaluating a response based on a given instruction (which may contain an Input). Provide a comprehensive feedback on the response quality based on the rules for evaluation. Follow this with a score between 1 and 5. Avoid generating any additional opening, closing, or explanations. 

Here are some rules of the evaluation:
(1) You should prioritize evaluating whether the output honestly/precisely/closely executes the instruction, then consider its helpfulness, accuracy, level of detail, harmlessness, etc. 
(2) Responses should NOT contain more/less than what the instruction asks for, as such responses do NOT precisely execute the instruction.
(3) You should avoid any potential bias and your judgment should be as objective as possible. Here is a potential source of bias:
- The length of the response should NOT affect your judgement, as a longer response does not necessarily correspond to a better response. When making your decision, evaluate if the response length is appropriate for the given instruction.{partial_response_note}

Your reply should strictly follow this format:
**Reasoning:** <Your feedback>

**Result:** <an integer between 1 and 5>

Here is the data:

Instruction:
```
{query_text}
```

Response:
```
{response}
```
""".strip()

PROMPT_SINGLE_RATING_ADDITIVE="""
You are tasked with evaluating a response based on a given instruction (which may contain an Input). Provide a comprehensive feedback on the response quality based on the rules for evaluation. Follow this with a score between 1 and 5. Avoid generating any additional opening, closing, or explanations. 

Here are some rules of the evaluation:
(1) You should prioritize evaluating whether the response satisfies the provided rubric. The basis of your score should depend exactly on the rubric.
(2) You should avoid any potential bias and your judgment should be as objective as possible. Here is a potential source of bias:
- The length of the response should NOT affect your judgement, as a longer response does not necessarily correspond to a better response. When making your decision, evaluate if the response length is appropriate for the given instruction.{partial_response_note}


Your reply should strictly follow this format:
**Reasoning:** <Your feedback>

**Result:** <an integer between 1 and 5>

Here is the data:

Instruction:
```
{query_text}
```

Response:
```
{response}
```

Score rubrics:
- Add one point if the response is relevant and provides some information related to the user's inquiry, even if it is incomplete or contains some irrelevant content.
- Add a second point if the response addresses a substantial portion of the user's question, but does not completely resolve the query or provide a direct answer.
- Add a third point if the response answers the basic elements of the user's question in a useful way, regardless of whether it seems to have been written by an AI Assistant or if it has elements typically found in blogs or search results.
- Add a fourth point if the response is clearly written from an AI Assistant's perspective, addressing the user's question directly and comprehensively, and is well-organized and helpful, even if there is slight room for improvement in clarity, conciseness or focus.
- Add a fifth point for a response that is impeccably tailored to the user's question by an AI Assistant, without extraneous information, reflecting expert knowledge, and demonstrating a high-quality, engaging, and insightful answer.
""".strip()

def parse_single_instance_rate_judgment(judge_output, return_critique):
    critique_judgement = judge_output.split('**Result:**')
    critique = critique_judgement[0].replace('**Reasoning:**', '').strip()
    judgement = critique_judgement[-1].strip()
    if return_critique:
        if judgement in ['1', '2', '3', '4', '5']:
            return int(judgement), critique
        else:
            return None, None
    else:
        if judgement in ['1', '2', '3', '4', '5']:
            return int(judgement)
        else:
            return None


def render_pairwise_prompt(query: Query, response0: Response, response1: Response, partial_response=False):
    prompt = PROMPT_PAIRWISE_COMPARISON.format(query_text=query.content, response_A=response0.content, response_B=response1.content,
                                                partial_response_note=get_partial_response(partial_response))

    return [dict(role='user', content=prompt)]

def render_single_instance_rate_prompt(query: Query, response: Response, which='single-likert', partial_response=False):
    assert not partial_response, 'Partial response template has not been implemented'
    assert which in ['single-likert', 'single-additive']
    if which == 'single-likert':
        prompt = PROMPT_SINGLE_RATING
    elif which == 'single-additive':
        prompt = PROMPT_SINGLE_RATING_ADDITIVE
    prompt = prompt.format(query_text=query.content, response=response.content, partial_response_note=get_partial_response(partial_response))
    return [dict(role='user', content=prompt)]
